{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300-images.tgz\n",
    "!wget https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300-human.tgz\n",
    "!tar zxvf *-images.tgz\n",
    "!tar zxvf *-human.tgz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "root_dir = \"./BSDS300\"\n",
    "train_data = os.path.join(root_dir, 'images/train')\n",
    "test_data = os.path.join(root_dir, 'images/test')\n",
    "labels = os.path.join(root_dir, 'human/color')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_labels(label_files):\n",
    "    \"\"\"Converts segmentation data from .seg files into np.array\"\"\"\n",
    "    meta = {}\n",
    "    data = []\n",
    "    with open(label_files, 'r') as file:\n",
    "        matcher = re.compile('(?P<seg>^[0-9 ]+)')\n",
    "        for line in file:\n",
    "            seg_match = matcher.search(line)\n",
    "            if seg_match:\n",
    "                string_segment = seg_match.group('seg').split(' ')\n",
    "                int_segment = np.asarray(string_segment, dtype=int)\n",
    "                data.append(int_segment)\n",
    "                continue\n",
    "            elif \"data\" not in line:\n",
    "                meta_data = line.strip('\\n').split(' ', 1)\n",
    "                index, value = meta_data[0], meta_data[1]\n",
    "                meta[index] = value\n",
    "    height, width = int(meta['height']), int(meta['width'])\n",
    "    seg_num = int(meta['segments'])\n",
    "    # print(f\"User id: {meta['user']}     Image id: {meta['image']}\")\n",
    "    # print(f\"Height: {height}       Width: {width}\")\n",
    "    segmentation = np.zeros((height, width))\n",
    "    for seg in data:\n",
    "        segmentation[seg[1], seg[2]:(seg[3] + 1)] = seg[0]\n",
    "    return segmentation, seg_num"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_image(seg_val, seg_max):\n",
    "    \"\"\"Creates an image using extracted segmentation data\"\"\"\n",
    "    seg_val = (seg_val / seg_max) * 255\n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(seg_val)\n",
    "    plt.show()\n",
    "\n",
    "# To test whether extracting segmentations is successful\n",
    "test = os.path.join(labels, '1105/15004.seg')\n",
    "segmentation, seg_num = extract_labels(test)\n",
    "create_image(segmentation, seg_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CropArrayCentre(object):\n",
    "    \"\"\"Custom transform to crop the centre of arrays (both images and segmentations)\"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[\"image\"], sample[\"label\"]\n",
    "        y, x, _ = image.shape\n",
    "        crop_y, crop_x = self.output_size\n",
    "        start_x, start_y = x // 2 - (crop_x // 2), y // 2 - (crop_y // 2)\n",
    "        image = image[start_y: start_y + crop_y, start_x: start_x + crop_x]\n",
    "        label = label[start_y: start_y + crop_y, start_x: start_x + crop_x]\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "class TwoTensor(object):\n",
    "    \"\"\"Custom transform to convert arrays (both images and segmentations) to tensors\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[\"image\"], sample[\"label\"]\n",
    "        image, label = torch.from_numpy(image).permute(2, 0, 1), torch.from_numpy(label)\n",
    "        return {'image': image, 'label': label}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Berkeley(Dataset):\n",
    "    \"\"\"Custom dataset containing training/test data + their respective labels\"\"\"\n",
    "    def __init__(self, image_files, label_files):\n",
    "        \"\"\"Images and labels are converted into np.arrays and listed in ascending index\n",
    "        :param image_files: Path to images\n",
    "        :param label_files: Path to segmentation labels\"\"\"\n",
    "        self.images, self.labels = self.array_from_path(image_files, label_files)\n",
    "        self.transform = transforms.Compose([\n",
    "            CropArrayCentre(321),  # Crops image + segmentation to uniform size\n",
    "            TwoTensor()  # Converts image + segmentation np.array to torch.tensor\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = {\"image\": self.images[index], \"label\": self.labels[index]}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def array_from_path(self, image_files, label_files):\n",
    "        images = sorted([img for img in os.listdir(image_files)])  # List in format .jpg\n",
    "        image_names = [os.path.splitext(img)[0] for img in images]  # List of image ids (sans .jpg)\n",
    "        ordered_files = {}\n",
    "        for root, user_folder, files in os.walk(label_files):\n",
    "            for file in files:\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                if file_name in image_names:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    seg, _ = extract_labels(file_path)\n",
    "                    ordered_files[file_name] = seg\n",
    "\n",
    "        images = [np.asarray(Image.open(os.path.join(image_files, img))) for img in images]\n",
    "        labels = [value for _, value in sorted(ordered_files.items(), key=lambda ele: ele[0])]\n",
    "        return images, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = Berkeley(train_data, labels)\n",
    "train_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=25,\n",
    "    shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = Berkeley(test_data, labels)\n",
    "test_loader = DataLoader(\n",
    "    test,\n",
    "    batch_size=25,\n",
    "    shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_images(sample_batched, i):\n",
    "    \"\"\"Show images and segmentation side-by-side in a batch of samples\"\"\"\n",
    "    images_batch, labels_batch = sample_batched['image'], sample_batched['label']\n",
    "    image_0, seg_0 = (images_batch[i].permute(1, 2, 0)), (labels_batch[i])\n",
    "    image_1, seg_1 = (images_batch[i+1].permute(1, 2, 0)), (labels_batch[i+1])\n",
    "    four_show = [image_0, seg_0, image_1, seg_1]\n",
    "\n",
    "    for ind in range(len(four_show)):\n",
    "        axarr[ind].imshow(four_show[ind])\n",
    "        axarr[ind].axis('off')\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['label'].size())\n",
    "    if i_batch == 5: # Stop and examine a specific batch\n",
    "        for ind in range(0,4,2):\n",
    "            f, axarr = plt.subplots(1,4, figsize=(15,15))\n",
    "            show_images(sample_batched, ind)\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "        break\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.stack = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channel, out_channel, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channel, out_channel, (3, 3)),\n",
    "            torch.nn.BatchNorm2d(out_channel),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stack(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, model):\n",
    "        x, features = self.contracting(model)\n",
    "        model = self.expanding(x, features)\n",
    "        return model\n",
    "\n",
    "    def contracting(self, model):\n",
    "        channels = [1, 64, 128, 256, 512, 1024]\n",
    "        features = []\n",
    "        for i in range(len(channels)-1):\n",
    "            block = Block(channels[i], channels[i+1])(model)\n",
    "            features.append(block)\n",
    "            model = torch.nn.MaxPool2d((2, 2))(block)\n",
    "        return model, features\n",
    "\n",
    "    def expanding(self, model, features):\n",
    "        channels = [1024, 512, 256, 128, 64]\n",
    "        for i in range(len(channels)-1):\n",
    "            block = Block(channels[i], channels[i+1])(model)\n",
    "            block = torch.cat([block, features[(len(channels)) - i - 2]])\n",
    "            model = torch.nn.ConvTranspose2d(channels[i+1], channels[i+1], (2, 2))(block)\n",
    "        return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}