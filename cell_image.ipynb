{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://cildata.crbs.ucsd.edu/ccdb//telescience/home/CCDB_DATA_USER.portal/P2043/Experiment_6835/Subject_6837/Tissue_6840/Microscopy_6843/MP6843_img_full.zip\n",
    "!wget https://cildata.crbs.ucsd.edu/ccdb//telescience/home/CCDB_DATA_USER.portal/P2043/Experiment_6835/Subject_6837/Tissue_6840/Microscopy_6843/MP6843_seg.zip\n",
    "!unzip -o -d ./images MP6843_img_full.zip\n",
    "!unzip -o -d ./labels MP6843_seg.zip\n",
    "!rm *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim, float\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "im_root, lb_root = \"./images\", \"./labels\"\n",
    "images = [join(im_root, im) for im in sorted(listdir(im_root)) if 'w1' in im]\n",
    "labels = [join(lb_root, lb) for lb in sorted(listdir(lb_root)) if '_01' in lb]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CellImages(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images, self.labels = images, labels\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        img = self.transform(Image.open(self.images[ind]))\n",
    "        lbl = self.transform(Image.open(self.labels[ind]))\n",
    "        return img, lbl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = DataLoader(CellImages(images[0:71], labels[0:31]), batch_size=20)\n",
    "test = DataLoader(CellImages(images[71:91], labels[31:41]), batch_size=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, sample in enumerate(train):\n",
    "    if i == 2:\n",
    "        img, lbl = sample\n",
    "        for ind in range(3):\n",
    "            f, ax = plt.subplots(1,2,figsize=(10,10))\n",
    "            ax[0].imshow(img[ind].numpy()[0])\n",
    "            ax[1].imshow(lbl[ind].numpy()[0])\n",
    "        plt.show()\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channel, out_channel, (3, 3)),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stack(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, channels, output_size, final_output=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down, self.up = channels, [c for c in reversed(channels[1:])]  # Number of feature channels\n",
    "        self.down_layers, self.up_layers = nn.ModuleList(), nn.ModuleList()\n",
    "        self.output_size = output_size\n",
    "        self.build_layers(final_output)  # Generate list of required modules\n",
    "        self.features = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.expansive(self.contracting(x))\n",
    "        output = F.interpolate(output, self.output_size)\n",
    "        return output\n",
    "\n",
    "    def build_layers(self, final):\n",
    "        # Generate sequence layer for the expansive & contracting path\n",
    "        for i in range(len(self.down) - 1):\n",
    "            self.down_layers.append(Block(self.down[i], self.down[i + 1]))\n",
    "            if i != max(range(len(self.down) - 1)):\n",
    "                self.down_layers.append(nn.MaxPool2d((2, 2), stride=2))  # 2x2 max pooling\n",
    "                self.up_layers.append(nn.ConvTranspose2d(self.up[i], self.up[i + 1], (2, 2), stride=2))\n",
    "                self.up_layers.append(Block(self.up[i], self.up[i + 1]))\n",
    "            else:\n",
    "                self.up_layers.append(nn.Conv2d(self.up[-1], final, (1, 1)))  # 1x1 convolutional layer\n",
    "\n",
    "    def contracting(self, model):\n",
    "        \"\"\"3x3 convolutions & ReLU followed by 2x2 max pooling for downsampling\"\"\"\n",
    "        for i in range(0, len(self.down_layers) - 1, 2):\n",
    "            block = self.down_layers[i](model)\n",
    "            self.features.append(block)  # Store block outputs for concatenation\n",
    "            model = self.down_layers[i + 1](block)\n",
    "        model = self.down_layers[-1](model)\n",
    "        return model\n",
    "\n",
    "    def expansive(self, model):\n",
    "        \"\"\"Upsampling followed by 3x3 convolutions & ReLU\"\"\"\n",
    "        for i in range(0, len(self.up_layers) - 1, 2):\n",
    "            block = self.up_layers[i](model)\n",
    "            feature = self.crop(self.features[::-1][int(i / 2)], block)\n",
    "            block = torch.cat([block, feature], dim=1)  # Double the number of feature channels\n",
    "            model = self.up_layers[i + 1](block)\n",
    "        model = self.up_layers[-1](model)\n",
    "        return model\n",
    "\n",
    "    def crop(self, feature, block):  # Necessary due to loss of border pixels every convolution\n",
    "        _, _, h, w = block.shape\n",
    "        feature = transforms.CenterCrop([h, w])(feature)\n",
    "        return feature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"Compare predictions and targets in mini-batches\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    for i, (img, lbl) in enumerate(dataloader):\n",
    "        img_batch, lbl_batch = img.to('cuda'), lbl.to('cuda')\n",
    "        pred = nn.Softmax(dim=1)(model(img_batch.float()))\n",
    "        loss = loss_fn(pred, lbl_batch.long())\n",
    "\n",
    "        # Backpropagation to adjust the weights and biases\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), (i + 1) * len(img_batch)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"Test the model with previously unseen, new images\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (img, lbl) in dataloader:\n",
    "            img_batch, lbl_batch = img.to('cuda'), lbl.to('cuda')\n",
    "            pred = nn.Softmax(dim=1)(model(img_batch.float()))\n",
    "            test_loss += loss_fn(pred, lbl_batch.long()).item()\n",
    "            correct += (pred.argmax(1) == lbl_batch).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= (size * output_size * output_size)  # Fraction of correctly classified pixels\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_size = (696, 520)\n",
    "channels = [1, 64, 128, 256, 512, 1024]\n",
    "handler = UNet(channels, output_size, 3).to('cuda')\n",
    "loss_fn = nn.BCELoss()\n",
    "optimiser = optim.SGD(handler.parameters(), lr=1e-3)\n",
    "epochs = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train_loop(train, handler, loss_fn, optimiser)\n",
    "    test_loop(test, handler, loss_fn)\n",
    "print(\"Done!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}